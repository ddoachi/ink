# E05-F02-T04: Incremental Search Optimization - Pre-Implementation Documentation

## Document Metadata
- **Task**: E05-F02-T04 - Incremental Search Optimization
- **Status**: Pre-Implementation Planning
- **Created**: 2025-12-26
- **Last Updated**: 2025-12-26
- **Author**: Claude Sonnet 4.5

---

## 1. Overview

### 1.1 Task Summary

Implement an optimization layer around SearchService that provides caching, progressive refinement, and query management for real-time incremental search. This enables smooth autocomplete UX as users type without overwhelming the search engine with redundant queries.

### 1.2 Problem Context

**The Incremental Search Challenge**:
```
User types: "c" → "cl" → "clk" → "clk_" → "clk_b" → "clk_bu" → "clk_buf"

Without optimization:
- 7 independent searches
- Each query scans entire dataset
- Total time: 7 × 80ms = 560ms
- CPU usage: High (7 full scans)

With optimization:
- First search: "c" (80ms, cache result)
- "cl": Refine cached "c" results (1ms)
- "clk": Refine cached "cl" results (1ms)
- "clk_": New search but cached (1ms if hit, 80ms if miss)
- Total time: ~160ms
- CPU usage: Low (1-2 full scans)
```

**Why This Matters**:
- Users type at 60-120 WPM = 5-10 chars/second
- Each keystroke triggers search
- Must feel instant (< 100ms perceived latency)
- Search engine sees 50+ queries in rapid succession

### 1.3 Success Criteria

- Perceived search latency < 100ms
- Cache hit rate > 50% for typical typing patterns
- Memory overhead < 10MB for cache
- CPU usage during typing < 10%
- Graceful handling of rapid query changes

---

## 2. Technical Approach

### 2.1 Optimization Strategies

**Three-Pronged Approach**:

1. **Result Caching**:
   - Store recent query results in LRU cache
   - Key: (query, filters) tuple
   - Value: List[SearchResult]
   - Instant retrieval for repeated queries

2. **Progressive Refinement**:
   - Detect prefix extensions ("cl" → "clk")
   - Filter previous results instead of new search
   - Only works for prefix queries (not wildcards)

3. **Query Debouncing** (UI layer):
   - Delay search execution by 150ms
   - Cancel if new keystroke arrives
   - Handled by SearchPanel QTimer (not this task)

### 2.2 Caching Strategy

**Cache Design**:
```python
cache_key = f"{query}|{filters_hash}"
cache_entry = SearchCache(
    query=query,
    filters=filters,
    results=[...],
    timestamp=time.time()
)
```

**Why Hash Filters**:
- Same query with different filters = different results
- Example: "clk" with only_cells vs "clk" with only_nets

**Cache Eviction**:
```
LRU eviction when cache_size exceeds limit
Time-based expiration (60 seconds)
```

**Why 60 Second TTL**:
- Design rarely changes during session
- Prevents stale results if design reloaded
- Balances memory usage vs hit rate

### 2.3 Progressive Refinement Algorithm

**Detection**:
```python
def is_prefix_extension(new_query: str, prev_query: str) -> bool:
    return (
        len(new_query) > len(prev_query) and
        new_query.startswith(prev_query) and
        len(prev_query) >= 2  # Minimum query length
    )
```

**Refinement**:
```python
def refine_results(
    prev_results: List[SearchResult],
    new_query: str
) -> List[SearchResult]:
    """Filter previous results to match extended query"""
    return [
        result for result in prev_results
        if new_query.lower() in result.name.lower()
    ]
```

**When Refinement Fails**:
- Query has wildcards (pattern changes)
- Previous query not in cache (evicted)
- Filter settings changed
- Fallback: Execute new search

**Example Refinement**:
```
Cached "cl" results: ["clk", "clk_buf", "clk_div2", "class", "clone"]

User types "clk":
Filter cached results → ["clk", "clk_buf", "clk_div2"]
(Instant, no search needed)
```

### 2.4 Architecture Design

**Layering**:
```
SearchPanel (UI)
    ↓ debouncing
IncrementalSearchHandler (Optimization) ← This task
    ↓ caching/refinement
SearchService (Core Logic)
    ↓
Trie/PatternMatcher (Infrastructure)
```

**Responsibilities**:
- **IncrementalSearchHandler**: Cache management, progressive refinement
- **SearchService**: Actual search logic (no caching)
- **SearchPanel**: Debouncing, UI updates

**Why Separate from SearchService**:
- SearchService remains pure, stateless
- Caching is optimization concern, not core logic
- Different cache strategies for different UIs possible

---

## 3. Implementation Strategy

### 3.1 Development Phases

#### Phase 1: Core Cache (1.5 hours)
**Deliverables**:
- `IncrementalSearchHandler` class
- `SearchCache` dataclass
- `_get_cached()` method
- `_cache_results()` method with LRU eviction
- Cache key generation

**Success Check**:
- Duplicate queries hit cache
- LRU eviction works correctly
- Cache size limit enforced

#### Phase 2: Progressive Refinement (1.5 hours)
**Deliverables**:
- `_is_prefix_extension()` detection
- `_refine_from_cache()` filtering
- Integration with search flow
- Unit tests for refinement logic

**Success Check**:
- Typing "c" → "cl" → "clk" uses refinement
- Refinement correctly filters results
- Falls back to search when refinement fails

#### Phase 3: Cache Management (0.5 hours)
**Deliverables**:
- Time-based cache expiration
- `clear_cache()` method
- `get_cache_stats()` for monitoring
- Cache key hashing for filters

**Success Check**:
- Expired entries removed
- Stats accurate
- Filter changes invalidate cache

#### Phase 4: Integration & Testing (0.5 hours)
**Deliverables**:
- Integration with SearchService
- Optional callback for async results
- Performance benchmarks
- Documentation

**Success Check**:
- Works seamlessly with SearchService
- Performance targets met
- Clean API for UI layer

### 3.2 File Organization

```
src/ink/application/services/
├── __init__.py
├── search_service.py
└── incremental_search.py      # IncrementalSearchHandler, SearchCache

tests/unit/application/services/
├── __init__.py
├── test_search_service.py
└── test_incremental_search.py # Cache and refinement tests

tests/benchmarks/
└── test_search_performance.py # Performance benchmarks
```

---

## 4. Design Decisions

### 4.1 Cache Size Limit

**Decision**: Default 50 entries, configurable

**Sizing Analysis**:
```
Per cache entry:
- Query string: ~50 bytes
- Filters: ~100 bytes
- Results (100 × SearchResult): ~10KB
- Metadata: ~50 bytes
Total: ~10KB per entry

50 entries × 10KB = 500KB
Conservative: 50 entries × 50KB (large results) = 2.5MB
```

**Why 50**:
- Covers typical search session (user tries ~10-20 queries)
- Includes refinement chain (e.g., "c", "cl", "clk", "clk_b", ...)
- Memory overhead acceptable
- LRU ensures most recent queries cached

**Configurable**: Power users can increase if needed

### 4.2 Cache Key Design

**Decision**: String concatenation of query + filter values

**Implementation**:
```python
def _make_cache_key(query: str, filters: SearchFilters) -> str:
    filter_key = f"{filters.include_cells}:{filters.include_nets}:"
                 f"{filters.include_ports}:{filters.include_pins}:"
                 f"{filters.max_results}"
    return f"{query}|{filter_key}"
```

**Alternatives**:
1. **Hash tuple**: `hash((query, filters))`
   - Pro: Faster comparison
   - Con: Hash collisions possible (rare but catastrophic)
2. **Pickle filters**: `query + pickle.dumps(filters)`
   - Pro: Handles complex filters
   - Con: Slower, more bytes
3. **String concat** (chosen)
   - Pro: Simple, readable, no collisions
   - Con: Slightly more memory

**Rationale**: Simplicity and correctness over marginal performance gains

### 4.3 Refinement vs New Search Tradeoff

**Decision**: Only refine if previous query cached and prefix extends it

**Why Not Always Refine**:
- Refinement may miss results if previous search was limited
- Example: "c" limited to 100 results, "clk" might need different 100
- Safety: Fall back to new search if uncertain

**Tradeoff**:
- Pro: Fast when works (1ms vs 80ms)
- Con: May miss edge cases

**Safety Check**:
```python
if len(prev_results) >= filters.max_results:
    # Previous search hit limit, may have missed results
    # Do new search to be safe
    return None  # Trigger new search
```

### 4.4 Callback vs Return Value

**Decision**: Support both synchronous return and async callback

**Why Callback**:
- Future: Background search threads
- UI can update progressively
- Non-blocking for large searches

**Implementation**:
```python
def search(
    query: str,
    on_results: Optional[Callable[[List[SearchResult]], None]] = None
) -> List[SearchResult]:
    results = self._do_search(query)

    if on_results:
        on_results(results)

    return results
```

**MVP**: Callback optional, mostly for future-proofing

---

## 5. Testing Strategy

### 5.1 Unit Tests

**Cache Hit/Miss Tests**:
```python
def test_cache_hit(handler):
    results1 = handler.search("clk")
    results2 = handler.search("clk")  # Should hit cache

    assert results1 == results2
    stats = handler.get_cache_stats()
    assert stats["cache_size"] > 0

def test_cache_miss_different_filters(handler):
    filters1 = SearchFilters(include_cells=True, include_nets=False)
    filters2 = SearchFilters(include_cells=False, include_nets=True)

    results1 = handler.search("clk", filters1)
    results2 = handler.search("clk", filters2)

    # Different results, different cache entries
    assert results1 != results2
    assert handler.get_cache_stats()["cache_size"] == 2
```

**Progressive Refinement Tests**:
```python
def test_progressive_typing(handler):
    """Simulate user typing 'clk_buf'"""
    queries = ["cl", "clk", "clk_", "clk_b", "clk_bu", "clk_buf"]

    for query in queries:
        results = handler.search(query)
        # Each should be progressively more refined

    # Should have used refinement for most queries
    stats = handler.get_cache_stats()
    assert stats["cache_size"] == len(queries)
```

**Cache Eviction Tests**:
```python
def test_cache_lru_eviction(handler):
    """Fill cache beyond limit, verify LRU eviction"""
    for i in range(60):  # More than cache_size=50
        handler.search(f"query_{i:02d}")

    stats = handler.get_cache_stats()
    assert stats["cache_size"] == handler._cache_size

def test_cache_expiration(handler):
    """Verify time-based expiration"""
    handler.search("clk")

    # Manually expire (for testing)
    for entry in handler._cache.values():
        entry.timestamp = time.time() - 65  # 65 seconds ago

    cached = handler._get_cached("clk", SearchFilters())
    assert cached is None  # Expired
```

### 5.2 Performance Benchmarks

**Cache Hit Speed**:
```python
def test_cache_hit_latency(handler, benchmark):
    # Warm cache
    handler.search("clk")

    # Benchmark cache hit
    result = benchmark(handler.search, "clk")
    assert benchmark.stats.median < 0.001  # < 1ms
```

**Refinement Speed**:
```python
def test_refinement_speed(handler, benchmark):
    handler.search("cl")  # Warm cache

    # Benchmark refinement
    result = benchmark(handler.search, "clk")
    assert benchmark.stats.median < 0.002  # < 2ms (faster than new search)
```

**Rapid Typing Simulation**:
```python
def test_typing_simulation(handler):
    """Simulate fast typing with realistic timing"""
    queries = ["c", "cl", "clk", "clk_", "clk_b", "clk_bu", "clk_buf"]

    start = time.time()
    for query in queries:
        handler.search(query)
        time.sleep(0.05)  # 50ms between keystrokes (realistic)
    elapsed = time.time() - start

    # Should complete in < 500ms
    assert elapsed < 0.5
```

### 5.3 Integration Tests

**With SearchService**:
```python
def test_integration_with_search_service(sample_design):
    service = SearchService(sample_design)
    handler = IncrementalSearchHandler(service)

    # Verify end-to-end flow
    results = handler.search("clk")
    assert len(results) > 0
    assert all(isinstance(r, SearchResult) for r in results)
```

---

## 6. Integration Points

### 6.1 Upstream Dependencies

**SearchService (E05-F02-T03)**:
```python
class IncrementalSearchHandler:
    def __init__(self, search_service: SearchService):
        self._search_service = search_service
```

**Expected API**:
```python
service.search(query: str, filters: SearchFilters) -> List[SearchResult]
```

### 6.2 Downstream Consumers

**SearchPanel UI (E05-F01)**:
```python
class SearchPanel:
    def __init__(self, search_service: SearchService):
        self._handler = IncrementalSearchHandler(search_service)
        self._debounce_timer = QTimer()
        self._debounce_timer.timeout.connect(self._execute_search)

    def on_text_changed(self, text: str):
        # Debounce: restart timer on each keystroke
        self._debounce_timer.stop()
        self._debounce_timer.start(150)  # 150ms delay

    def _execute_search(self):
        query = self._search_input.text()
        results = self._handler.search(query)  # Uses cache/refinement
        self._display_results(results)
```

**Typical Flow**:
```
User types "c" → QTimer starts (150ms)
User types "l" → QTimer restarts (150ms)
User types "k" → QTimer restarts (150ms)
150ms elapses → _execute_search()
Handler.search("clk") → Check cache → Return results
```

---

## 7. Risk Analysis

### 7.1 Performance Risks

#### Risk 1: Cache Hit Rate Lower Than Expected
**Impact**: Medium (more searches than expected, slower UX)
**Probability**: Low (typing patterns are predictable)

**Mitigation**:
- Instrument cache with hit/miss counters
- Monitor in real usage
- Tune cache size if needed

**Contingency**:
- Increase cache size to 100 entries
- Implement smarter cache key (ignore filter differences if results same)

#### Risk 2: Refinement Produces Incorrect Results
**Impact**: High (users see wrong results)
**Probability**: Low (simple substring matching)

**Mitigation**:
- Comprehensive unit tests
- Conservative refinement (only for simple prefix extensions)
- Fallback to new search if uncertain

**Contingency**:
- Disable refinement, rely on cache only
- Add validation: compare refined results to new search (in tests)

#### Risk 3: Memory Leak from Unbounded Cache
**Impact**: High (OOM after extended usage)
**Probability**: Low (LRU eviction should prevent)

**Mitigation**:
- LRU eviction from day 1
- Memory profiling in tests
- Monitoring in production

**Contingency**:
- Reduce cache size
- Clear cache periodically (e.g., every 100 searches)

### 7.2 Integration Risks

#### Risk 4: Race Conditions with Rapid Queries
**Impact**: Medium (results out of order, confusing UX)
**Probability**: Low (Python GIL, single-threaded)

**Mitigation**:
- Synchronous search in MVP (no threads)
- Clear cache on design reload
- Debouncing in UI prevents most races

**Contingency**:
- Add query sequence numbers, ignore stale results
- Lock cache updates if threading added

---

## 8. Future Enhancements (Post-MVP)

### 8.1 Advanced Caching

**Predictive Prefetching**:
```python
# User typed "clk", likely to type "clk_"
# Pre-compute "clk_" search in background
def prefetch_likely_queries(query: str):
    likely = [query + "_", query + "0", query + "1"]
    for q in likely:
        threading.Thread(target=self._search_service.search, args=(q,)).start()
```

**Persistent Cache**:
```python
# Save cache to disk, restore on startup
def save_cache(path: str):
    with open(path, 'wb') as f:
        pickle.dump(self._cache, f)

def load_cache(path: str):
    with open(path, 'rb') as f:
        self._cache = pickle.load(f)
```

### 8.2 Intelligent Refinement

**Result Expansion**:
```python
# If refinement produces too few results, supplement with new search
refined = self._refine_from_cache(query)
if len(refined) < 10:
    new_results = self._search_service.search(query)
    refined.extend(new_results)
```

**Scoring Adjustment**:
```python
# Boost scores for refined results (user is drilling down)
for result in refined_results:
    result.match_quality *= 1.1  # 10% boost
```

### 8.3 Analytics

**Usage Metrics**:
```python
class SearchMetrics:
    cache_hits: int
    cache_misses: int
    refinements_used: int
    avg_results_per_query: float
    common_queries: Counter[str]

def get_metrics() -> SearchMetrics:
    # For analyzing search patterns, improving algorithms
```

---

## 9. Acceptance Checklist

### 9.1 Code Completeness

- [ ] `IncrementalSearchHandler` class implemented
- [ ] `SearchCache` dataclass defined
- [ ] `_get_cached()` retrieves from cache
- [ ] `_cache_results()` stores with LRU eviction
- [ ] `_is_prefix_extension()` detects refinement opportunities
- [ ] `_refine_from_cache()` filters previous results
- [ ] Time-based cache expiration (60s TTL)
- [ ] `clear_cache()` method
- [ ] `get_cache_stats()` returns metrics
- [ ] Type hints on all public methods
- [ ] Comprehensive docstrings

### 9.2 Testing

- [ ] Unit tests for cache hit/miss
- [ ] Unit tests for progressive refinement
- [ ] Unit tests for LRU eviction
- [ ] Unit tests for expiration
- [ ] Performance benchmarks for cache hit speed
- [ ] Rapid typing simulation test
- [ ] Integration with SearchService tested
- [ ] Test coverage > 85%

### 9.3 Performance

- [ ] Cache hit latency < 1ms
- [ ] Refinement latency < 2ms
- [ ] Memory overhead < 10MB
- [ ] Rapid typing completes in < 500ms
- [ ] Cache hit rate > 50% in typing tests

### 9.4 Documentation

- [ ] Docstrings explain optimization strategies
- [ ] Cache eviction policy documented
- [ ] Refinement algorithm explained
- [ ] Usage examples in docstrings

---

## 10. Open Questions

### 10.1 Design Questions

**Q1**: Should cache persist across design reloads?
- **Context**: User closes design, reopens same file
- **Options**:
  - A) Clear cache on reload (safe, simple)
  - B) Persist cache (faster, but may be stale)
- **Recommendation**: Option A for MVP

**Q2**: Should we cache negative results (no matches)?
- **Context**: Query "xyz" finds nothing, cache that?
- **Options**:
  - A) Cache empty results (consistent)
  - B) Don't cache empty (save memory)
- **Recommendation**: Option A (consistency, memory negligible)

**Q3**: What if filters change during typing?
- **Context**: User toggles "only cells" while typing
- **Options**:
  - A) Clear cache on filter change
  - B) Maintain separate cache per filter combo
  - C) Invalidate affected entries only
- **Recommendation**: Option B (already handled by cache key)

### 10.2 Integration Questions

**Q4**: Should IncrementalSearchHandler be in application or presentation layer?
- **Current**: Application layer
- **Reasoning**: Optimization is application concern, not UI-specific
- **Recommendation**: Keep in application, UI can choose to use it or not

**Q5**: How to handle very long query strings (> 100 chars)?
- **Context**: Pathological case, unlikely in practice
- **Recommendation**: No special handling, cache key can be long

---

## Appendix A: Cache Performance Analysis

### Expected Cache Hit Rates

**Typing Pattern**: User types "clk_buf"

| Keystroke | Query | Cache Hit? | Refinement? | Latency |
|-----------|-------|------------|-------------|---------|
| c | "c" | Miss | No | 80ms |
| l | "cl" | Miss | Yes | 2ms |
| k | "clk" | Miss | Yes | 2ms |
| _ | "clk_" | Miss | Yes | 2ms |
| b | "clk_b" | Miss | Yes | 2ms |
| u | "clk_bu" | Miss | Yes | 2ms |
| f | "clk_buf" | Miss | Yes | 2ms |

**Total**: 80ms + 6×2ms = 92ms (vs 7×80ms = 560ms without optimization)

**Cache Hit Scenario**: User types "clk", backspaces, types "clk" again

| Action | Query | Cache Hit? | Latency |
|--------|-------|------------|---------|
| Type "clk" | "clk" | Miss | 80ms |
| Backspace | "cl" | Hit (from earlier) | 1ms |
| Type "k" | "clk" | Hit | 1ms |

**Total**: 82ms for sequence with repetition

### Memory Calculation

**Per Cache Entry**:
```
query: "clk_buf_main" (50 bytes)
filters: SearchFilters (100 bytes)
results: 100 × SearchResult
  - Each SearchResult: ~100 bytes
  - Total: 10KB
timestamp: 8 bytes

Total per entry: ~10KB
```

**50 Entries**:
```
50 × 10KB = 500KB (typical)
50 × 50KB = 2.5MB (worst case, large result sets)
```

**Conclusion**: Memory overhead negligible (< 3MB)

---

## Appendix B: Example Usage

### Basic Usage

```python
design = Design.from_file("design.ckt")
service = SearchService(design)
handler = IncrementalSearchHandler(service)

# First search (cache miss)
results1 = handler.search("clk")  # 80ms

# Same query (cache hit)
results2 = handler.search("clk")  # 1ms

# Extended query (refinement)
results3 = handler.search("clk_b")  # 2ms
```

### With Filters

```python
filters = SearchFilters(include_cells=True, include_nets=False)

# Each filter combo has separate cache
results_cells = handler.search("clk", filters)
results_all = handler.search("clk")  # Different cache key
```

### Cache Management

```python
# Check cache statistics
stats = handler.get_cache_stats()
print(f"Cache size: {stats['cache_size']}/{stats['max_cache_size']}")
print(f"Last query: {stats['last_query']}")

# Clear cache (e.g., on design reload)
handler.clear_cache()
```

### With Callback (Future)

```python
def on_results(results: List[SearchResult]):
    print(f"Found {len(results)} results")
    update_ui(results)

# Async result delivery
handler.search("clk", on_results=on_results)
```

---

## Document Revision History

| Date | Version | Author | Changes |
|------|---------|--------|---------|
| 2025-12-26 | 1.0 | Claude Sonnet 4.5 | Initial pre-implementation documentation |

---

**End of Pre-Implementation Documentation**
