# Pre-Implementation Documentation: E01-F01-T01 - CDL Lexer and Tokenizer

## Overview

### Problem Context
CDL (Circuit Description Language) files contain gate-level netlist information in a SPICE-like format. Before we can parse the semantic content of subcircuits and instances, we need a lexical analyzer that:
- Handles multi-line statements with `+` continuation
- Strips comments starting with `*`
- Classifies lines by type (SUBCKT, ENDS, INSTANCE, etc.)
- Preserves line numbers for error reporting

This lexer provides the foundation for all downstream parsing components. Without proper tokenization, the subcircuit and instance parsers would need to duplicate line handling logic.

### Goals
Build an iterator-based lexer that yields clean, classified tokens from CDL files while handling continuation and comments transparently.

---

## Implementation Approach

### High-Level Strategy

**Architecture Pattern**: Iterator pattern with line buffering for continuation handling

**Key Components**:
1. `LineType` enum - Classification of CDL line types
2. `CDLToken` dataclass - Immutable token with metadata
3. `CDLLexer` class - Main tokenization engine with lazy evaluation

**Processing Pipeline**:
```
File Lines → Buffer → Continuation Join → Comment Strip → Classify → Yield Token
```

### Algorithm Design

**Tokenization Flow**:
1. Read file line by line (generator for memory efficiency)
2. Check for continuation (`+` prefix on next line)
3. Buffer and join continued lines
4. Strip inline comments (everything after `*`)
5. Classify line type based on content patterns
6. Yield `CDLToken` with original line number

**Classification Rules**:
- `.SUBCKT` → SUBCKT
- `.ENDS` → ENDS
- `X<name>` → INSTANCE
- `M<name>` → TRANSISTOR (skip in MVP)
- `*` → COMMENT
- Empty/whitespace → BLANK
- Other → UNKNOWN

### Data Structures

```python
class LineType(Enum):
    SUBCKT = "SUBCKT"
    ENDS = "ENDS"
    INSTANCE = "INSTANCE"
    TRANSISTOR = "TRANSISTOR"
    COMMENT = "COMMENT"
    BLANK = "BLANK"
    UNKNOWN = "UNKNOWN"

@dataclass
class CDLToken:
    line_num: int       # 1-indexed, first line of multi-line statement
    line_type: LineType
    content: str        # Cleaned, joined content
    raw: str           # Original first line (for error context)
```

---

## Key Design Decisions

### Decision 1: Iterator vs List Return

**Options**:
- A) Return `List[CDLToken]` - Load entire file into memory
- B) Return `Iterator[CDLToken]` - Lazy evaluation

**Recommendation**: Option B (Iterator)

**Rationale**:
- Large CDL files (100K+ lines) would consume excessive memory
- Downstream parsers can process tokens incrementally
- Supports streaming for UI progress updates
- Minimal performance overhead with Python generators

**Trade-offs**:
- Cannot seek backward without re-parsing
- Must collect into list for two-pass parsing (acceptable - done by parser integration layer)

### Decision 2: Continuation Handling

**Options**:
- A) Look-ahead buffering - Read next line to check for `+`
- B) State machine - Track "in continuation" flag
- C) Regex-based multi-line matching

**Recommendation**: Option A (Look-ahead buffering)

**Rationale**:
- Simple and readable implementation
- Handles arbitrary continuation depth
- Works naturally with file iteration

**Implementation**:
```python
def _handle_continuation(self, lines_iter):
    buffer = []
    for line in lines_iter:
        if line.lstrip().startswith('+'):
            buffer.append(line.lstrip()[1:])  # Remove + and join
        else:
            if buffer:
                yield ' '.join(buffer)
                buffer = [line]
            else:
                buffer = [line]
```

### Decision 3: Comment Stripping Strategy

**Options**:
- A) Simple split on `*` - Fast but doesn't handle strings
- B) Regex with escape handling - Handles escaped `*` in strings
- C) State machine parser - Full string literal support

**Recommendation**: Option A (Simple split) for MVP

**Rationale**:
- CDL format rarely uses string literals
- Performance critical for large files
- Can upgrade to Option B if needed

**Edge Case Handling**:
- If `*` appears in instance/net names, user must use escaped format (rare)
- Document this limitation in parser documentation

### Decision 4: Line Number Tracking

**Options**:
- A) Store first line number of multi-line statement
- B) Store line number range (start, end)
- C) Store only end line number

**Recommendation**: Option A (First line number)

**Rationale**:
- Most natural for error reporting ("error starts at line N")
- Simplest to implement
- Sufficient for debugging

**Enhancement**: Store `end_line_num` as optional field for future detailed error context

---

## Dependencies and Integration Points

### Upstream Dependencies
**None** - Operates directly on file system

**Requirements**:
- Python `pathlib.Path` for cross-platform file handling
- Standard library `enum` and `dataclasses`

### Downstream Consumers

**E01-F01-T02 (Subcircuit Parser)**:
- Consumes `SUBCKT` and `ENDS` tokens
- Requires cleaned content without comments
- Needs accurate line numbers for error reporting

**E01-F01-T03 (Instance Parser)**:
- Consumes `INSTANCE` tokens
- Expects continuation already handled
- Uses line numbers for warning/error context

**E01-F01-T05 (Parser Integration)**:
- Iterates over all tokens
- May collect into list for two-pass parsing
- Handles progress reporting based on token count

### Interface Contract

**Input**: `Path` to `.ckt` file
**Output**: `Iterator[CDLToken]`

**Guarantees**:
- Tokens yielded in file order
- Line numbers strictly increasing (modulo continuation)
- Content is whitespace-normalized (single spaces)
- Comments fully removed from content
- Raw field preserves original for debugging

---

## Testing Strategy

### Unit Test Coverage

**Core Functionality**:
1. `test_classify_subckt_line` - Verify `.SUBCKT` detection
2. `test_classify_ends_line` - Verify `.ENDS` detection
3. `test_classify_instance_line` - Verify `X*` detection
4. `test_classify_comment_line` - Verify `*` comment detection
5. `test_classify_blank_line` - Verify empty/whitespace detection

**Comment Handling**:
6. `test_strip_inline_comment` - Remove `* comment` from end of line
7. `test_strip_full_comment_line` - Handle entire line as comment
8. `test_preserve_no_comment` - Don't modify lines without comments

**Continuation Handling**:
9. `test_handle_single_continuation` - Two-line statement
10. `test_handle_multiple_continuation` - 3+ line statement
11. `test_handle_no_continuation` - Normal single line
12. `test_continuation_preserves_line_number` - First line number retained

**Integration**:
13. `test_tokenize_sample_file` - End-to-end on realistic file
14. `test_tokenize_prd_appendix_a` - PRD sample file

**Edge Cases**:
15. `test_empty_file` - Zero tokens yielded
16. `test_only_comments` - All COMMENT tokens
17. `test_only_blank_lines` - All BLANK tokens
18. `test_crlf_line_endings` - Windows line endings
19. `test_lf_line_endings` - Unix line endings
20. `test_very_long_continuation` - 10+ continuation lines
21. `test_whitespace_only_line` - Classify as BLANK

### Test Data Strategy

**Fixtures**:
- `sample_cdl_minimal.ckt` - 10 lines, basic coverage
- `sample_cdl_continuation.ckt` - Multiple continuation examples
- `sample_cdl_comments.ckt` - Various comment placements
- `sample_cdl_prd.ckt` - PRD Appendix A sample

**Parametrized Tests**:
```python
@pytest.mark.parametrize("line,expected_type", [
    (".SUBCKT INV A Y", LineType.SUBCKT),
    (".ENDS INV", LineType.ENDS),
    ("XI1 A Y INV", LineType.INSTANCE),
    ("* comment", LineType.COMMENT),
    ("", LineType.BLANK),
    ("   ", LineType.BLANK),
])
def test_classification(line, expected_type):
    lexer = CDLLexer(...)
    assert lexer._classify_line(line) == expected_type
```

### Performance Testing

**Benchmark**: Large file (100K lines) should tokenize in <1 second

```python
def test_performance_large_file(benchmark, tmp_path):
    # Generate 100K line file
    lines = [".SUBCKT INV A Y", ".ENDS INV"] * 50000
    file_path = tmp_path / "large.ckt"
    file_path.write_text("\n".join(lines))

    lexer = CDLLexer(file_path)
    tokens = benchmark(lambda: list(lexer.tokenize()))

    assert len(tokens) == 100000
```

---

## Risks and Considerations

### Risk 1: Character Encoding Issues
**Likelihood**: Low
**Impact**: Medium

**Description**: CDL files may contain non-ASCII characters (e.g., vendor-specific annotations)

**Mitigation**:
- Open files with `encoding='utf-8', errors='ignore'`
- Document supported encoding in API
- Log warning on decode errors

### Risk 2: Very Long Lines
**Likelihood**: Medium
**Impact**: Low

**Description**: Some tools generate very long instance lines with 100+ connections

**Mitigation**:
- Python string handling is efficient for long strings
- No explicit line length limit in MVP
- Monitor memory usage in performance tests

### Risk 3: Ambiguous Comment Syntax
**Likelihood**: Low
**Impact**: Medium

**Description**: `*` in net names could be confused with comments

**Mitigation**:
- Document that `*` in names must be escaped
- Add warning in lexer if suspicious pattern detected
- Rely on CDL generation tools to escape properly

### Risk 4: Platform Line Ending Differences
**Likelihood**: Low
**Impact**: Low

**Description**: Files may have CRLF (Windows) vs LF (Unix) endings

**Mitigation**:
- Python `open()` handles both with universal newline mode
- Explicit test coverage for both formats
- No manual line ending handling needed

### Risk 5: Memory Usage with Continuation
**Likelihood**: Low
**Impact**: Medium

**Description**: Extremely long continued statements could buffer many lines

**Mitigation**:
- Set practical limit (e.g., 100 continuation lines)
- Raise error if limit exceeded
- Document limitation

### Implementation Complexity
**Overall**: Low

**Rationale**:
- Well-defined problem domain
- Minimal external dependencies
- Straightforward algorithm
- Good test coverage achievable

### Integration Challenges
**Level**: Low

**Considerations**:
- No complex state shared with downstream parsers
- Clean interface (iterator of tokens)
- Line number preservation sufficient for error context

---

## Success Metrics

### Functional Requirements
- [ ] 95%+ code coverage
- [ ] All PRD sample file tokens correctly classified
- [ ] Zero false positives on comment detection
- [ ] Continuation joins produce correct output

### Non-Functional Requirements
- [ ] Parse 100K lines in <1 second
- [ ] Memory usage <50MB for 1M line file (iterator efficiency)
- [ ] Zero crashes on malformed input (graceful error handling)

### Quality Gates
- [ ] All unit tests pass
- [ ] Mypy type checking passes with strict mode
- [ ] Ruff linting passes with no warnings
- [ ] Documentation complete with usage examples

---

## Implementation Checklist

### Phase 1: Core Implementation (2-3 hours)
- [ ] Create `LineType` enum
- [ ] Create `CDLToken` dataclass
- [ ] Implement `CDLLexer.__init__`
- [ ] Implement `_classify_line` method
- [ ] Implement `_strip_comment` method
- [ ] Implement basic `tokenize` generator

### Phase 2: Continuation Handling (1-2 hours)
- [ ] Implement `_handle_continuation` logic
- [ ] Integrate continuation into `tokenize`
- [ ] Test multi-line statements

### Phase 3: Testing (1-2 hours)
- [ ] Write classification tests
- [ ] Write comment stripping tests
- [ ] Write continuation tests
- [ ] Create test fixture files
- [ ] Add edge case tests

### Phase 4: Validation (0.5-1 hour)
- [ ] Run full test suite
- [ ] Performance benchmark
- [ ] Type checking
- [ ] Linting
- [ ] Documentation review

**Total Estimated Time**: 4-6 hours (matches spec estimate)
